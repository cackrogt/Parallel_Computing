import os
import random
import numpy as np
import tensorflow as tf
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import train_test_split

# -----------------------------
# CONFIGURATION
# -----------------------------
DATA_PATH = "your_data_path"  # CHANGE this to your dataset folder
NGRAM_RANGE = (1, 2)
TOP_K = 20000
TOKEN_MODE = 'word'
MIN_DOCUMENT_FREQUENCY = 2

# -----------------------------
# DATA LOADING FROM FOLDERS
# -----------------------------
def load_custom_text_classification_dataset(data_path, seed=123):
    train_texts, train_labels = [], []
    test_texts, test_labels = [], []

    for split in ['train', 'test']:
        split_path = os.path.join(data_path, split)
        class_dirs = sorted(os.listdir(split_path))

        for idx, class_name in enumerate(class_dirs):
            class_dir = os.path.join(split_path, class_name)
            if not os.path.isdir(class_dir):
                continue
            for fname in sorted(os.listdir(class_dir)):
                if fname.endswith('.txt'):
                    with open(os.path.join(class_dir, fname), encoding='utf-8') as f:
                        text = f.read()
                    if split == 'train':
                        train_texts.append(text)
                        train_labels.append(idx)
                    else:
                        test_texts.append(text)
                        test_labels.append(idx)

    # Shuffle with seed
    random.seed(seed)
    combined = list(zip(train_texts, train_labels))
    random.shuffle(combined)
    train_texts, train_labels = zip(*combined)

    return ((list(train_texts), np.array(train_labels)),
            (test_texts, np.array(test_labels)))

# -----------------------------
# N-GRAM VECTORIZATION
# -----------------------------
def ngram_vectorize(train_texts, train_labels, val_texts):
    kwargs = {
        'ngram_range': NGRAM_RANGE,
        'dtype': 'int32',
        'strip_accents': 'unicode',
        'decode_error': 'replace',
        'analyzer': TOKEN_MODE,
        'min_df': MIN_DOCUMENT_FREQUENCY,
    }

    vectorizer = TfidfVectorizer(**kwargs)
    x_train = vectorizer.fit_transform(train_texts)
    x_val = vectorizer.transform(val_texts)

    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))
    selector.fit(x_train, train_labels)
    x_train = selector.transform(x_train).astype('float32')
    x_val = selector.transform(x_val).astype('float32')
    return x_train, x_val

# -----------------------------
# MODEL BUILDING
# -----------------------------
def _get_last_layer_units_and_activation(num_classes):
    return num_classes, 'softmax'

def mlp_model(layers, units, dropout_rate, input_shape, num_classes):
    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dropout(rate=dropout_rate, input_shape=input_shape))

    for _ in range(layers - 1):
        model.add(tf.keras.layers.Dense(units=units, activation='relu'))
        model.add(tf.keras.layers.Dropout(rate=dropout_rate))

    model.add(tf.keras.layers.Dense(units=op_units, activation=op_activation))
    return model

# -----------------------------
# TRAINING FUNCTION
# -----------------------------
def train_ngram_model(data,
                      learning_rate=1e-3,
                      epochs=10,
                      batch_size=128,
                      layers=2,
                      units=64,
                      dropout_rate=0.2):

    (train_texts, train_labels), (val_texts, val_labels) = data
    num_classes = len(np.unique(train_labels))

    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]
    if len(unexpected_labels):
        raise ValueError(f"Unexpected label values in validation set: {unexpected_labels}")

    # Vectorize
    x_train, x_val = ngram_vectorize(train_texts, train_labels, val_texts)

    # Build model
    model = mlp_model(layers=layers,
                      units=units,
                      dropout_rate=dropout_rate,
                      input_shape=x_train.shape[1:],
                      num_classes=num_classes)

    # Compile model
    loss = 'binary_crossentropy' if num_classes == 2 else 'sparse_categorical_crossentropy'
    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]

    # Train model
    history = model.fit(
        x_train,
        train_labels,
        validation_data=(x_val, val_labels),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=2
    )

    print(f"âœ… Final val accuracy: {history.history['val_accuracy'][-1]:.4f}")
    model.save("mlp_ngram_model.h5")
    return model

# -----------------------------
# MAIN SCRIPT
# -----------------------------
if __name__ == "__main__":
    # Load from folder
    (train_texts, train_labels), (test_texts, test_labels) = load_custom_text_classification_dataset(DATA_PATH)

    # Split train into train/val
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels
    )

    # Train model
    model = train_ngram_model(
        data=((train_texts, train_labels), (val_texts, val_labels)),
        learning_rate=1e-3,
        epochs=10,
        batch_size=128,
        layers=2,
        units=64,
        dropout_rate=0.2
    )
