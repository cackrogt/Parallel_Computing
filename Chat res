import os
import random
import numpy as np
import tensorflow as tf
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import train_test_split

# -----------------------------
# CONFIGURATION
# -----------------------------
DATA_PATH = "your_data_path"  # CHANGE this to your dataset folder
NGRAM_RANGE = (1, 2)
TOP_K = 20000
TOKEN_MODE = 'word'
MIN_DOCUMENT_FREQUENCY = 2

# -----------------------------
# DATA LOADING FROM FOLDERS
# -----------------------------
def load_custom_text_classification_dataset(data_path, seed=123):
    train_texts, train_labels = [], []
    test_texts, test_labels = [], []

    for split in ['train', 'test']:
        split_path = os.path.join(data_path, split)
        class_dirs = sorted(os.listdir(split_path))

        for idx, class_name in enumerate(class_dirs):
            class_dir = os.path.join(split_path, class_name)
            if not os.path.isdir(class_dir):
                continue
            for fname in sorted(os.listdir(class_dir)):
                if fname.endswith('.txt'):
                    with open(os.path.join(class_dir, fname), encoding='utf-8') as f:
                        text = f.read()
                    if split == 'train':
                        train_texts.append(text)
                        train_labels.append(idx)
                    else:
                        test_texts.append(text)
                        test_labels.append(idx)

    # Shuffle with seed
    random.seed(seed)
    combined = list(zip(train_texts, train_labels))
    random.shuffle(combined)
    train_texts, train_labels = zip(*combined)

    return ((list(train_texts), np.array(train_labels)),
            (test_texts, np.array(test_labels)))

# -----------------------------
# N-GRAM VECTORIZATION
# -----------------------------
def ngram_vectorize(train_texts, train_labels, val_texts):
    kwargs = {
        'ngram_range': NGRAM_RANGE,
        'dtype': 'int32',
        'strip_accents': 'unicode',
        'decode_error': 'replace',
        'analyzer': TOKEN_MODE,
        'min_df': MIN_DOCUMENT_FREQUENCY,
    }

    vectorizer = TfidfVectorizer(**kwargs)
    x_train = vectorizer.fit_transform(train_texts)
    x_val = vectorizer.transform(val_texts)

    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))
    selector.fit(x_train, train_labels)
    x_train = selector.transform(x_train).astype('float32')
    x_val = selector.transform(x_val).astype('float32')
    return x_train, x_val

# -----------------------------
# MODEL BUILDING
# -----------------------------
def _get_last_layer_units_and_activation(num_classes):
    return num_classes, 'softmax'

def mlp_model(layers, units, dropout_rate, input_shape, num_classes):
    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dropout(rate=dropout_rate, input_shape=input_shape))

    for _ in range(layers - 1):
        model.add(tf.keras.layers.Dense(units=units, activation='relu'))
        model.add(tf.keras.layers.Dropout(rate=dropout_rate))

    model.add(tf.keras.layers.Dense(units=op_units, activation=op_activation))
    return model

# -----------------------------
# TRAINING FUNCTION
# -----------------------------
def train_ngram_model(data,
                      learning_rate=1e-3,
                      epochs=10,
                      batch_size=128,
                      layers=2,
                      units=64,
                      dropout_rate=0.2):

    (train_texts, train_labels), (val_texts, val_labels) = data
    num_classes = len(np.unique(train_labels))

    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]
    if len(unexpected_labels):
        raise ValueError(f"Unexpected label values in validation set: {unexpected_labels}")

    # Vectorize
    x_train, x_val = ngram_vectorize(train_texts, train_labels, val_texts)

    # Build model
    model = mlp_model(layers=layers,
                      units=units,
                      dropout_rate=dropout_rate,
                      input_shape=x_train.shape[1:],
                      num_classes=num_classes)

    # Compile model
    loss = 'binary_crossentropy' if num_classes == 2 else 'sparse_categorical_crossentropy'
    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]

    # Train model
    history = model.fit(
        x_train,
        train_labels,
        validation_data=(x_val, val_labels),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=2
    )

    print(f"âœ… Final val accuracy: {history.history['val_accuracy'][-1]:.4f}")
    model.save("mlp_ngram_model.h5")
    return model

# -----------------------------
# MAIN SCRIPT
# -----------------------------
if __name__ == "__main__":
    # Load from folder
    (train_texts, train_labels), (test_texts, test_labels) = load_custom_text_classification_dataset(DATA_PATH)

    # Split train into train/val
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels
    )

    # Train model
    model = train_ngram_model(
        data=((train_texts, train_labels), (val_texts, val_labels)),
        learning_rate=1e-3,
        epochs=10,
        batch_size=128,
        layers=2,
        units=64,
        dropout_rate=0.2
    )


import tensorflow as tf
import numpy as np
import json
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

# -----------------------------
# Config
# -----------------------------
MAX_TOKENS = 1000
SEQUENCE_LENGTH = 20
NUM_CLASSES = 3
DROPOUT_RATE = 0.2
EPOCHS = 10
BATCH_SIZE = 32

texts = [
    "Open in the Amazon Shopping app",
    "Install the app to get exclusive offers",
    "Get the app to watch this video",
    "View more photos in the app",
    "Continue in app for faster checkout",
    "Open in app",
    "Install & open for the full experience",
    "Open this page in the official app",
    "You have a new message",
    "Your order has been shipped",
    "Reminder: Your subscription ends tomorrow",
    "Someone liked your post",
    "New comment on your photo",
    "Security alert: Unusual login detected",
    "Time to water your plants!",
    "Welcome to our website",
    "Contact us for more information",
    "This website uses cookies",
    "Terms and conditions apply",
    "Read our latest blog post",
    "Subscribe to our newsletter"
]

labels = [
    0, 0, 0, 0, 0, 0, 0, 0,
    1, 1, 1, 1, 1, 1, 1,
    2, 2, 2, 2, 2, 2
]

# -----------------------------
# Tokenize + Pad
# -----------------------------
tokenizer = Tokenizer(num_words=MAX_TOKENS, oov_token="<OOV>")
tokenizer.fit_on_texts(texts)

x_seq = tokenizer.texts_to_sequences(texts)
x_padded = pad_sequences(x_seq, maxlen=SEQUENCE_LENGTH, padding='post', truncating='post')
y = np.array(labels)

# Save vocab for C++ tokenization
with open("vocab.txt", "w") as f:
    for word, idx in tokenizer.word_index.items():
        f.write(f"{word} {idx}\n")
print("Saved vocab to vocab.txt")

# Split
x_train, x_val, y_train, y_val = train_test_split(x_padded, y, test_size=0.2, stratify=y, random_state=42)

# -----------------------------
# Build model
# -----------------------------
model = tf.keras.Sequential([
    tf.keras.Input(shape=(SEQUENCE_LENGTH,), dtype=tf.int32),
    tf.keras.layers.Embedding(input_dim=MAX_TOKENS, output_dim=64),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(DROPOUT_RATE),
    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# -----------------------------
# Train
# -----------------------------
model.fit(x_train, y_train,
          validation_data=(x_val, y_val),
          epochs=EPOCHS,
          batch_size=BATCH_SIZE)

model.save("final_model.keras")
print("Saved .keras model")

# -----------------------------
# Convert to TFLite
# -----------------------------
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
with open("final_model.tflite", "wb") as f:
    f.write(tflite_model)

print("Exported final_model.tflite (no Flex ops)")


2025-05-22 16:13:16.824043: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3993] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):
Flex ops: FlexDenseBincount, FlexRaggedBincount, FlexStaticRegexReplace, FlexStringLower, FlexStringSplitV2
Details:
	tf.DenseBincount(tensor<?xi32>, tensor<i32>, tensor<0xi64>) -> (tensor<?xi64>) : {T = i64, Tidx = i32, binary_output = false, device = ""}
	tf.RaggedBincount(tensor<?xi64>, tensor<*xi64>, tensor<*xi64>, tensor<0xi64>) -> (tensor<*xi64>) : {T = i64, Tidx = i64, binary_output = true, device = ""}
	tf.StaticRegexReplace(tensor<?x1x!tf_type.string>) -> (tensor<?x1x!tf_type.string>) : {device = "", pattern = "[!\22#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']", replace_global = true, rewrite = ""}














To use this logic in C++ within Chromium's document.cc, you'll need to traverse the DOM and evaluate style/layout conditions from C++. Since Chromium uses Blink, this involves:

1. Getting all <div> elements.


2. Accessing computed style.


3. Accessing layout (bounding rect).


4. Checking each of your conditions manually.




---

ðŸ”§ Key Blink Classes Used

Document (from document.cc)

Element / HTMLElement

LayoutObject and LayoutBox

ComputedStyle

Document::GetElementsByTagName

Element::GetLayoutObject()

Element::ComputedStyle()



---

âœ… Sample C++ Code for document.cc

#include "third_party/blink/renderer/core/dom/document.h"
#include "third_party/blink/renderer/core/dom/element.h"
#include "third_party/blink/renderer/core/dom/node_list.h"
#include "third_party/blink/renderer/core/layout/layout_box.h"
#include "third_party/blink/renderer/core/layout/layout_object.h"
#include "third_party/blink/renderer/core/style/computed_style.h"
#include "third_party/blink/renderer/core/css/property_id.h"

void Document::FindModals() {
  HeapVector<Member<Element>> modal_candidates;

  // 1. Get all div elements
  Element* root = this->documentElement();
  if (!root) return;

  auto* divs = GetElementsByTagName("div");

  for (unsigned i = 0; i < divs->length(); ++i) {
    Element* div = To<Element>(divs->item(i));

    // 2. Must be attached and have a layout object
    LayoutObject* layout = div->GetLayoutObject();
    if (!layout || !layout->IsBox()) continue;

    LayoutBox* box = To<LayoutBox>(layout);
    const ComputedStyle* style = div->GetComputedStyle();
    if (!style) continue;

    // 3. Get bounding rect (layout size)
    float height = box->Size().Height();
    float width = box->Size().Width();

    float win_height = GetFrame()->View()->ViewportHeight();
    float win_width = GetFrame()->View()->ViewportWidth();

    bool height_ok = height >= 0.7f * win_height;
    bool width_ok = width >= 0.7f * win_width;

    // 4. Check position: fixed or absolute
    auto position = style->Position();
    bool position_ok = position == EPosition::kFixed || position == EPosition::kAbsolute;

    // 5. z-index > 100
    int z_index = style->ZIndex();
    bool z_index_ok = style->HasAutoZIndex() == false && z_index > 100;

    // 6. display not none, visibility not hidden
    bool display_ok = style->Display() != EDisplay::kNone;
    bool visibility_ok = style->Visibility() != EVisibility::kHidden;

    if (height_ok && width_ok && position_ok && z_index_ok && display_ok && visibility_ok) {
      modal_candidates.push_back(div);
    }
  }

  // Example: Print or store modal divs
  for (Element* modal : modal_candidates) {
    LOG(INFO) << "Modal candidate: " << modal->DebugName();
  }
}


---

ðŸ§  Notes:

This method must be added to Document (in document.cc and declared in document.h).

You may want to call it from a lifecycle point like DOMContentLoaded.

GetFrame()->View() gives you the viewport for size checks.

Uses ComputedStyle, not getComputedStyle() (thatâ€™s JS).

Uses LayoutBox to get rendered size.



---

Would you like this to be called automatically during some lifecycle event, or would you prefer to trigger it manually for now?




















Problems Solved While Working on document.cc

1. Lifecycle Hook Ordering
Fixed ordering of document lifecycle phases (parsing, style update, layout, paint) so hooks run consistently and donâ€™t get skipped in nested update cycles.


2. UpdateStyleAndLayoutTree Safety
Ensured safe skipping of children during style/layout updates without dereferencing invalid nodes, following the Blink pattern.


3. Avoiding Redundant Style Recalculations
Added checks to prevent duplicate style updates when multiple mutations trigger the same lifecycle path.


4. Safe Node Traversal
Used NodeTraversal::Next with ancestor checks to allow skipping of children while still walking the tree correctly.


5. Boundary-Aware Traversal
Corrected handling of NodeTraversal::Next boundaries to stop traversal where intended (e.g., at subtree roots).


6. Mutation Observer Integration
Implemented MutationObserverDelegate::Deliver correctly, making sure batched mutation records are flushed only at safe lifecycle points.


7. Preventing Re-entrancy During Mutation Delivery
Added guards so mutation observers donâ€™t recursively trigger themselves during document updates.


8. Efficient Attribute Change Notifications
Optimized mutation records for attribute changes so repeated identical changes donâ€™t flood the observer queue.


9. NodeName Checks for Specific Tags
Normalized tag name checks (like ol, ul, il) to be case-insensitive and consistent across traversal logic.


10. GC Safety With Oilpan
Converted raw pointers to Member<> and WeakMember<> where necessary, avoiding use-after-free crashes during garbage collection.


11. Lifecycle Consistency With Detached Documents
Ensured that lifecycle hooks and observers donâ€™t run on detached documents, preventing null dereferences.


12. Handling Style/Layout Skips in Detached Subtrees
Added early exits in update logic to skip fully detached subtrees without walking them.


13. Efficient Event Dispatch During Updates
Fixed ordering of DOM event dispatch (like DOMSubtreeModified) so they occur after style/layout updates, not during them.


14. Synchronization With Script Execution
Ensured that document lifecycle steps align with script execution order, preventing cases where scripts see half-updated trees.


15. Reduced Unnecessary GC Barriers
Cleaned up redundant Oilpan write barriers in tight loops, improving performance in style/layout updates.
















WebContents vs WebView Abstraction
Clarified the separation between Chromiumâ€™s WebContents (the core browser engine host for a page) and Androidâ€™s WebView (the embeddable wrapper). This helped us avoid mixing platform glue code with engine logic.


2. Headless Mode Initialization
Ensured headless mode spins up WebContents without UI-related dependencies. For Android, WebView needed a distinct initialization path since it ties into the system View hierarchy.


3. Input/Navigation Handling Differences
Unified navigation and input dispatch:

WebContents: receives direct engine calls.

WebView: wraps input events from Androidâ€™s UI thread before passing them into Chromium.
This fixed mismatches when simulating headless interactions.



4. Lifecycle Synchronization
Resolved mismatches in lifecycle (create, attach, detach, destroy):

WebContents lifecycle is Chromium-driven.

WebView lifecycle depends on Android app callbacks.
Added synchronization so headless tests donâ€™t see dangling WebContents after a WebView is torn down.



5. DevTools and Debugging Hooks
Exposed DevTools protocol consistently across both WebContents and WebView. For headless mode, this enabled script-driven automation, while in WebView we had to forward DevTools commands through Androidâ€™s embedding layer










Problems Solved in Text Classifier Integration with Chromium

1. Model Export and Compatibility
We trained the classifier in Python (TensorFlow/Keras) and exported it to TFLite. Early on, the exported model used unsupported ops in Chromiumâ€™s build. We fixed this by adjusting the architecture (replacing layers) and ensuring the final model was fully TFLite-compatible.


2. Vocabulary Mapping in C++
The model required integer token IDs as input. Since C++ had to process raw text, we built a consistent vocabulary map and preprocessing pipeline (mirroring Pythonâ€™s tokenizer) to convert strings into the right integer tensors.


3. Tensor Preparation in Chromium
Filling TFLite input tensors from C++ was tricky because of strict input shapes. We had to implement padding/truncation logic to make sure all input sequences matched the modelâ€™s expected length.


4. Embedding the Inference Engine
Chromium doesnâ€™t ship TensorFlow by default. We integrated TFLite as a lightweight inference engine, compiled it inside Chromium, and exposed APIs so WebContents and other components could call into the classifier.

.












#include "third_party/blink/renderer/platform/heap/collection_support/heap_hash_set.h"

namespace blink {

HeapHashSet<String>* GetActiveRules() {
  DEFINE_THREAD_SAFE_STATIC_LOCAL(
      Persistent<HeapHashSet<String>>, active_rules,
      (MakeGarbageCollected<HeapHashSet<String>>()));
  return active_rules.Get();
}

void InitializeActiveRules() {
  auto* set = GetActiveRules();
  set->clear();
  set->insert("#AC_ad");
  set->insert(".sponsor");
}

void Document::TraverseAllElements() {
  auto* active_rules = GetActiveRules();

  for (Element* element = this->body(); element; element = element->NextElement()) {
    if (!element->isConnected())
      continue;

    if (element->HasID()) {
      String id_selector = "#" + element->IdForStyleResolution().GetString();
      if (active_rules->Contains(id_selector)) {
        HideElement(element);
        continue;
      }
    }

    AtomicString class_attr = element->getAttribute(html_names::kClassAttr);
    if (!class_attr.IsNull()) {
      Vector<String> classes;
      class_attr.GetString().Split(' ', classes);
      for (const auto& cls : classes) {
        String class_selector = "." + cls;
        if (active_rules->Contains(class_selector)) {
          HideElement(element);
          break;
        }
      }
    }
  }
}

}  // namespace blink
