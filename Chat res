import os
import random
import numpy as np
import tensorflow as tf
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import train_test_split

# -----------------------------
# CONFIGURATION
# -----------------------------
DATA_PATH = "your_data_path"  # CHANGE this to your dataset folder
NGRAM_RANGE = (1, 2)
TOP_K = 20000
TOKEN_MODE = 'word'
MIN_DOCUMENT_FREQUENCY = 2

# -----------------------------
# DATA LOADING FROM FOLDERS
# -----------------------------
def load_custom_text_classification_dataset(data_path, seed=123):
    train_texts, train_labels = [], []
    test_texts, test_labels = [], []

    for split in ['train', 'test']:
        split_path = os.path.join(data_path, split)
        class_dirs = sorted(os.listdir(split_path))

        for idx, class_name in enumerate(class_dirs):
            class_dir = os.path.join(split_path, class_name)
            if not os.path.isdir(class_dir):
                continue
            for fname in sorted(os.listdir(class_dir)):
                if fname.endswith('.txt'):
                    with open(os.path.join(class_dir, fname), encoding='utf-8') as f:
                        text = f.read()
                    if split == 'train':
                        train_texts.append(text)
                        train_labels.append(idx)
                    else:
                        test_texts.append(text)
                        test_labels.append(idx)

    # Shuffle with seed
    random.seed(seed)
    combined = list(zip(train_texts, train_labels))
    random.shuffle(combined)
    train_texts, train_labels = zip(*combined)

    return ((list(train_texts), np.array(train_labels)),
            (test_texts, np.array(test_labels)))

# -----------------------------
# N-GRAM VECTORIZATION
# -----------------------------
def ngram_vectorize(train_texts, train_labels, val_texts):
    kwargs = {
        'ngram_range': NGRAM_RANGE,
        'dtype': 'int32',
        'strip_accents': 'unicode',
        'decode_error': 'replace',
        'analyzer': TOKEN_MODE,
        'min_df': MIN_DOCUMENT_FREQUENCY,
    }

    vectorizer = TfidfVectorizer(**kwargs)
    x_train = vectorizer.fit_transform(train_texts)
    x_val = vectorizer.transform(val_texts)

    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))
    selector.fit(x_train, train_labels)
    x_train = selector.transform(x_train).astype('float32')
    x_val = selector.transform(x_val).astype('float32')
    return x_train, x_val

# -----------------------------
# MODEL BUILDING
# -----------------------------
def _get_last_layer_units_and_activation(num_classes):
    return num_classes, 'softmax'

def mlp_model(layers, units, dropout_rate, input_shape, num_classes):
    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dropout(rate=dropout_rate, input_shape=input_shape))

    for _ in range(layers - 1):
        model.add(tf.keras.layers.Dense(units=units, activation='relu'))
        model.add(tf.keras.layers.Dropout(rate=dropout_rate))

    model.add(tf.keras.layers.Dense(units=op_units, activation=op_activation))
    return model

# -----------------------------
# TRAINING FUNCTION
# -----------------------------
def train_ngram_model(data,
                      learning_rate=1e-3,
                      epochs=10,
                      batch_size=128,
                      layers=2,
                      units=64,
                      dropout_rate=0.2):

    (train_texts, train_labels), (val_texts, val_labels) = data
    num_classes = len(np.unique(train_labels))

    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]
    if len(unexpected_labels):
        raise ValueError(f"Unexpected label values in validation set: {unexpected_labels}")

    # Vectorize
    x_train, x_val = ngram_vectorize(train_texts, train_labels, val_texts)

    # Build model
    model = mlp_model(layers=layers,
                      units=units,
                      dropout_rate=dropout_rate,
                      input_shape=x_train.shape[1:],
                      num_classes=num_classes)

    # Compile model
    loss = 'binary_crossentropy' if num_classes == 2 else 'sparse_categorical_crossentropy'
    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]

    # Train model
    history = model.fit(
        x_train,
        train_labels,
        validation_data=(x_val, val_labels),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=2
    )

    print(f"âœ… Final val accuracy: {history.history['val_accuracy'][-1]:.4f}")
    model.save("mlp_ngram_model.h5")
    return model

# -----------------------------
# MAIN SCRIPT
# -----------------------------
if __name__ == "__main__":
    # Load from folder
    (train_texts, train_labels), (test_texts, test_labels) = load_custom_text_classification_dataset(DATA_PATH)

    # Split train into train/val
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels
    )

    # Train model
    model = train_ngram_model(
        data=((train_texts, train_labels), (val_texts, val_labels)),
        learning_rate=1e-3,
        epochs=10,
        batch_size=128,
        layers=2,
        units=64,
        dropout_rate=0.2
    )


import tensorflow as tf
import numpy as np
import json
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

# -----------------------------
# Config
# -----------------------------
MAX_TOKENS = 1000
SEQUENCE_LENGTH = 20
NUM_CLASSES = 3
DROPOUT_RATE = 0.2
EPOCHS = 10
BATCH_SIZE = 32

texts = [
    "Open in the Amazon Shopping app",
    "Install the app to get exclusive offers",
    "Get the app to watch this video",
    "View more photos in the app",
    "Continue in app for faster checkout",
    "Open in app",
    "Install & open for the full experience",
    "Open this page in the official app",
    "You have a new message",
    "Your order has been shipped",
    "Reminder: Your subscription ends tomorrow",
    "Someone liked your post",
    "New comment on your photo",
    "Security alert: Unusual login detected",
    "Time to water your plants!",
    "Welcome to our website",
    "Contact us for more information",
    "This website uses cookies",
    "Terms and conditions apply",
    "Read our latest blog post",
    "Subscribe to our newsletter"
]

labels = [
    0, 0, 0, 0, 0, 0, 0, 0,
    1, 1, 1, 1, 1, 1, 1,
    2, 2, 2, 2, 2, 2
]

# -----------------------------
# Tokenize + Pad
# -----------------------------
tokenizer = Tokenizer(num_words=MAX_TOKENS, oov_token="<OOV>")
tokenizer.fit_on_texts(texts)

x_seq = tokenizer.texts_to_sequences(texts)
x_padded = pad_sequences(x_seq, maxlen=SEQUENCE_LENGTH, padding='post', truncating='post')
y = np.array(labels)

# Save vocab for C++ tokenization
with open("vocab.txt", "w") as f:
    for word, idx in tokenizer.word_index.items():
        f.write(f"{word} {idx}\n")
print("Saved vocab to vocab.txt")

# Split
x_train, x_val, y_train, y_val = train_test_split(x_padded, y, test_size=0.2, stratify=y, random_state=42)

# -----------------------------
# Build model
# -----------------------------
model = tf.keras.Sequential([
    tf.keras.Input(shape=(SEQUENCE_LENGTH,), dtype=tf.int32),
    tf.keras.layers.Embedding(input_dim=MAX_TOKENS, output_dim=64),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(DROPOUT_RATE),
    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# -----------------------------
# Train
# -----------------------------
model.fit(x_train, y_train,
          validation_data=(x_val, y_val),
          epochs=EPOCHS,
          batch_size=BATCH_SIZE)

model.save("final_model.keras")
print("Saved .keras model")

# -----------------------------
# Convert to TFLite
# -----------------------------
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
with open("final_model.tflite", "wb") as f:
    f.write(tflite_model)

print("Exported final_model.tflite (no Flex ops)")


2025-05-22 16:13:16.824043: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3993] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):
Flex ops: FlexDenseBincount, FlexRaggedBincount, FlexStaticRegexReplace, FlexStringLower, FlexStringSplitV2
Details:
	tf.DenseBincount(tensor<?xi32>, tensor<i32>, tensor<0xi64>) -> (tensor<?xi64>) : {T = i64, Tidx = i32, binary_output = false, device = ""}
	tf.RaggedBincount(tensor<?xi64>, tensor<*xi64>, tensor<*xi64>, tensor<0xi64>) -> (tensor<*xi64>) : {T = i64, Tidx = i64, binary_output = true, device = ""}
	tf.StaticRegexReplace(tensor<?x1x!tf_type.string>) -> (tensor<?x1x!tf_type.string>) : {device = "", pattern = "[!\22#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']", replace_global = true, rewrite = ""}
